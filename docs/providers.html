<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title><p>Providers</p> - PersistenceAI Documentation</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        :root {
            --bg-primary: #0d1117;
            --bg-secondary: #161b22;
            --text-primary: #d4d4d4;
            --text-secondary: #858585;
            --accent: #4ec9b0;
            --border: #3e3e3e;
            --code-bg: #161b22;
            --code-border: #30363d;
        }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            line-height: 1.6;
            padding: 2rem;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
        }
        .back-link {
            display: inline-block;
            margin-bottom: 2rem;
            color: var(--text-secondary);
            text-decoration: none;
        }
        .back-link:hover {
            color: var(--accent);
        }
        h1 {
            color: var(--accent);
            margin-bottom: 1.5rem;
            font-size: 2.5rem;
            border-bottom: 2px solid var(--border);
            padding-bottom: 1rem;
        }
        h2 {
            color: var(--accent);
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-size: 1.75rem;
        }
        h3 {
            color: var(--text-primary);
            margin-top: 1.5rem;
            margin-bottom: 0.75rem;
            font-size: 1.25rem;
        }
        p {
            margin-bottom: 1rem;
            color: var(--text-primary);
        }
        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1rem;
        }
        li {
            margin-bottom: 0.5rem;
            color: var(--text-primary);
        }
        code {
            background: var(--code-bg);
            border: 1px solid var(--code-border);
            border-radius: 4px;
            padding: 0.2rem 0.4rem;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            color: var(--accent);
        }
        pre {
            background: var(--code-bg);
            border: 1px solid var(--code-border);
            border-radius: 8px;
            padding: 1rem;
            overflow-x: auto;
            margin-bottom: 1rem;
        }
        pre code {
            background: none;
            border: none;
            padding: 0;
            color: var(--text-primary);
            white-space: pre;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 1.5rem;
        }
        th, td {
            border: 1px solid var(--border);
            padding: 0.75rem;
            text-align: left;
        }
        th {
            background: var(--bg-secondary);
            color: var(--accent);
            font-weight: 600;
        }
        td {
            color: var(--text-primary);
        }
        a {
            color: var(--accent);
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        blockquote {
            border-left: 4px solid var(--accent);
            padding-left: 1rem;
            margin-left: 0;
            margin-bottom: 1rem;
            color: var(--text-secondary);
            font-style: italic;
        }
        hr {
            border: none;
            border-top: 1px solid var(--border);
            margin: 2rem 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <a href="index.html" class="back-link">‚Üê Back to Documentation</a>
        <h1><p>Providers</p></h1>

<p>PersistenceAI supports multiple LLM providers. You can configure API keys for any provider you want to use.</p>

<h2><p>Connecting a Provider</p></h2>

<ul><li><p>Run the <code>/connect</code> command in the TUI</p></li>
<li><p>Select a provider from the list</p></li>
<li><p>Follow the provider-specific authentication steps</p></li>
<li><p>Enter your API key when prompted</p></li>

</ul><pre><code>/connect</code></pre>

<h2><p>Supported Providers</p></h2>

<p>PersistenceAI supports all major LLM providers:</p>

<ul><li><strong>OpenAI</strong> - GPT and more</li>
<li><strong>Anthropic</strong> - Claude models</li>
<li><strong>Google</strong> - Gemini models</li>
<li><strong>Ollama</strong> - Local models (self-hosted)</li>
<li><strong>PersistenceAI Zero</strong> - Curated list of tested models</li>
<li><p>And many more...</p></li>

</ul><h2><p>Provider Configuration</p></h2>

<p>Provider configurations are stored in your global config file:</p>

<ul><li><strong>Windows</strong>: <code>%USERPROFILE%\.config\pai\pai.json</code></li>
<li><strong>Linux/macOS</strong>: <code>~/.config/pai/pai.json</code></li>

</ul><h3><p>Example Configuration</p></h3>

<pre><code>{
  &quot;providers&quot;: {
    &quot;openai&quot;: {
      &quot;apiKey&quot;: &quot;sk-...&quot;
    },
    &quot;anthropic&quot;: {
      &quot;apiKey&quot;: &quot;sk-ant-...&quot;
    },
    &quot;ollama&quot;: {
      &quot;baseURL&quot;: &quot;http://localhost:11434&quot;
    }
  }
}</code></pre>

<h2><p>Ollama (Self-Hosted)</p></h2>

<p>PersistenceAI has excellent support for Ollama, allowing you to run models locally without API costs.</p>

<h3><p>Setup</p></h3>

<ul><li><p>Install Ollama from <a href="https://ollama.ai">ollama.ai</a></p></li>
<li><p>Pull the models you want to use:</p></li>
   </ul><pre><code>ollama pull llama2
   ollama pull codellama</code></pre>
<ul><li><p>Connect to Ollama in PersistenceAI:</p></li>
   </ul><pre><code>/connect</code></pre>
<p>Select "Ollama" and enter your base URL (default: <code>http://localhost:11434</code>)</p>

<h3><p>Recommended Models</p></h3>

<ul><li><strong>cogito:3b</strong> - Fast, lightweight for simple tasks</li>
<li><strong>qwen2.5-coder:4b</strong> - Good balance of speed and capability</li>
<li><strong>qwen2.5-coder:7b</strong> - More capable for complex tasks</li>
<li><strong>qwen2.5-coder:13b+</strong> - Maximum capability</li>

</ul><h2><p>PersistenceAI Zero</p></h2>

<p>PersistenceAI Zero is a curated list of models that have been tested and verified by the PersistenceAI team.</p>

<ul><li><p>Run <code>/connect</code> and select "PersistenceAI Zero"</p></li>
<li><p>Visit the authentication page</p></li>
<li><p>Sign in and get your API key</p></li>
<li><p>Paste the key in PersistenceAI</p></li>

</ul><h2><p>Model Selection</p></h2>

<p>You can switch between models using:</p>

<ul><li><strong>Keyboard</strong>: <code>F2</code> to cycle through recently used models</li>
<li><strong>Command</strong>: <code>/models</code> to see all available models</li>
<li><strong>Shortcut</strong>: <code><leader>M</code> (default: <code>Ctrl+X M</code>)</li>

</ul><h2><p>Provider-Specific Notes</p></h2>

<h3><p>OpenAI</p></h3>

<ul><li><p>Requires API key from <a href="https://platform.openai.com">platform.openai.com</a></p></li>
<li><p>Supports GPT and other models</p></li>
<li><p>Usage-based billing</p></li>

</ul><h3><p>Anthropic</p></h3>

<ul><li><p>Requires API key from <a href="https://console.anthropic.com">console.anthropic.com</a></p></li>
<li><p>Supports Claude 3 models</p></li>
<li><p>Usage-based billing</p></li>

</ul><h3><p>Google</p></h3>

<ul><li><p>Requires API key from <a href="https://makersuite.google.com/app/apikey">Google AI Studio</a></p></li>
<li><p>Supports Gemini models</p></li>
<li><p>Usage-based billing</p></li>

</ul><h3><p>Ollama</p></h3>

<ul><li><p>Free and self-hosted</p></li>
<li><p>No API key required</p></li>
<li><p>Runs models locally on your machine</p></li>
<li><p>Best for privacy-sensitive projects</p></li>

</ul><hr>

<p>For more information, see:</p>
<ul><li><a href="./config.md">Configuration</a> - Full configuration options</li>
<li><a href="./models.md">Models</a> - Model selection and usage</li>
</ul>
    </div>
</body>
</html>